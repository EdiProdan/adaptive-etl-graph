# config.yaml
# Wikipedia Data Collection Configuration

data_collection:
  # Number of pages to collect for base graph
  target_pages: 10000

  # Output paths
  output_dir: "data/input/pages"
  base_pages_file: "base_pages.json"

  # API settings
  rate_limit_delay: 0.1  # seconds between requests
  batch_save_interval: 100  # save progress every N pages

  # Page filtering
  exclude_patterns:
    - "Main_Page"
    - "Special:"
    - "File:"
    - "Category:"
    - "Template:"
    - "Wikipedia:"
    - "Help:"
    - "Portal:"
    - "List_of"
    - "disambiguation"

logging:
  level: "INFO"
  log_dir: "data/logs"
  log_file: "wikipedia_collection.log"

# Future phases (for reference)
phases:
  phase1_extraction:
    wikipedia_api: true
    image_ocr: false  # Will add later

  phase2_symbolization:
    sbert_model: "all-MiniLM-L6-v2"
    similarity_threshold: 0.7
    batch_size: 100

  phase3_construction:
    neo4j_uri: "bolt://localhost:7687"
    batch_load_size: 1000